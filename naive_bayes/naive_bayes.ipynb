{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_header, tokens_header, message_header = \"cluster\", \"tokens\", \"message\"\n",
    "cluster1, cluster2, cluster3, cluster4, cluster5 = \"NoInfo\", \"Test\", \"Bomb\", \"Npe\", \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_data(input_file: str):\n",
    "    with open(input_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    elements_to_delete = []\n",
    "    for each in data:\n",
    "        if each[message_header] == \"(no message)\":\n",
    "            elements_to_delete.append(each)\n",
    "    for e in elements_to_delete:\n",
    "        data.remove(e)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def cluster_all_messages(raw_data: List[Dict]) -> DataFrame:\n",
    "    result = []\n",
    "    deleted_tokens, added_tokens = \"deletedTokens\", \"addedTokens\"\n",
    "    for current in raw_data:\n",
    "        if \"cleanup\" in current[message_header] or \"minor\" in current[message_header] or \\\n",
    "            current[message_header] == \"fix\" or current[message_header] == \"fixes\" or \\\n",
    "            current[message_header] == \"several fixes\" or current[message_header] == \"minor fixes\" or \\\n",
    "            current[message_header] == \"yet another inference bug\" or \\\n",
    "            current[message_header] == \"some improvements (igor e. mikhailuk)\" or \\\n",
    "            current[message_header] == \"no changes actually\" or \\\n",
    "            current[message_header] == \"clean up\" or current[message_header] == \"cosmetics\":\n",
    "            \n",
    "            cluster = cluster1\n",
    "        elif \"test\" in current[message_header] and \\\n",
    "            current[message_header] != \"This commit was manufactured by cvs2svn to create branch 'testing'.\" and \\\n",
    "            current[message_header] != \"This commit was manufactured by cvs2svn to create branch 'test'.\" and \\\n",
    "            current[message_header] != \"This commit was manufactured by cvs2svn to create branch 'test_for_ven'.\":\n",
    "\n",
    "            cluster = cluster2\n",
    "        elif \"bomb\" in current[message_header]:\n",
    "            cluster = cluster3\n",
    "        elif \"npe\" in current[message_header] or \"null\" in current[message_header]:\n",
    "            cluster = cluster4\n",
    "        else:\n",
    "            cluster = cluster5\n",
    "        \n",
    "        if current[deleted_tokens]:  # list with tokens not empty\n",
    "            result.append({cluster_header: cluster,\n",
    "                           tokens_header: current[deleted_tokens],\n",
    "                           message_header: current[message_header]\n",
    "                           })\n",
    "        if current[added_tokens]:  # list with tokens not empty\n",
    "            result.append({cluster_header: cluster,\n",
    "                       tokens_header: current[added_tokens],\n",
    "                       message_header: current[message_header]\n",
    "                       })\n",
    "    return DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### get data; IDEA ###################\n",
    "parent_dir = \"/Users/natalia.murycheva/Documents/gitCommitMessageCollectorStorage\"\n",
    "git_dir_name = \"intellij\"\n",
    "git_dir = os.path.join(parent_dir, git_dir_name)\n",
    "json_with_diffs = f\"{git_dir_name}_diff_blobs.json\"\n",
    "json_with_diffs = os.path.join(parent_dir, json_with_diffs)\n",
    "\n",
    "raw_data = load_and_filter_data(json_with_diffs)\n",
    "df = cluster_all_messages(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total shape \t\t(101026, 3)\n",
      "size cluster1 \tNoInfo\t4025\t%3.9841228990556887\n",
      "size cluster2 \tTest\t3711\t%3.6733118207194186\n",
      "size cluster3 \tBomb\t421\t%0.4167244075782472\n",
      "size cluster4 \tNpe\t643\t%0.6364698196503871\n",
      "size cluster5 \tOther\t92226\t%91.28937105299626\n"
     ]
    }
   ],
   "source": [
    "print(f\"total shape \\t\\t{df.shape}\")\n",
    "\n",
    "total_size = df.shape[0]\n",
    "\n",
    "size1 = (df[cluster_header] == cluster1).sum()\n",
    "size2 = (df[cluster_header] == cluster2).sum()\n",
    "size3 = (df[cluster_header] == cluster3).sum()\n",
    "size4 = (df[cluster_header] == cluster4).sum()\n",
    "size5 = (df[cluster_header] == cluster5).sum()\n",
    "\n",
    "print(f\"size cluster1 \\t{cluster1}\\t{size1}\\t%{(size1/total_size)*100}\")\n",
    "print(f\"size cluster2 \\t{cluster2}\\t{size2}\\t%{(size2/total_size)*100}\")\n",
    "print(f\"size cluster3 \\t{cluster3}\\t{size3}\\t%{(size3/total_size)*100}\")\n",
    "print(f\"size cluster4 \\t{cluster4}\\t{size4}\\t%{(size4/total_size)*100}\")\n",
    "print(f\"size cluster5 \\t{cluster5}\\t{size5}\\t%{(size5/total_size)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### split data ###################\n",
    "\n",
    "msg_train , msg_test, y_train, y_test = train_test_split(df[message_header], df[cluster_header],\n",
    "                                                         test_size=0.3, random_state=442)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(msg_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       1.00      0.23      0.37       131\n",
      "      NoInfo       0.96      0.77      0.85      1154\n",
      "         Npe       1.00      0.31      0.47       204\n",
      "       Other       0.97      1.00      0.98     27728\n",
      "        Test       0.90      0.68      0.78      1091\n",
      "\n",
      "    accuracy                           0.97     30308\n",
      "   macro avg       0.97      0.60      0.69     30308\n",
      "weighted avg       0.97      0.97      0.97     30308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################### classififcation results ###################\n",
    "predicted = text_clf.predict(msg_test)\n",
    "np.mean(predicted == y_test)    \n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result = 0.03583387395378652\n"
     ]
    }
   ],
   "source": [
    "################### bleu score ###################\n",
    "total_score = 0.\n",
    "for msg, cluster in zip(msg_test, predicted):\n",
    "    if cluster == cluster1:\n",
    "        candidate = \"cleanup\"\n",
    "    elif cluster == cluster2:\n",
    "        candidate = \"test\"\n",
    "    elif cluster == cluster3:\n",
    "        candidate = \"bomb\"\n",
    "    elif cluster == cluster4:\n",
    "        candidate = \"npe\"\n",
    "    else:\n",
    "        candidate = \"smth was changed\"\n",
    "    \n",
    "    score = sentence_bleu([msg], candidate, weights=(1, 0, 0, 0))\n",
    "    if candidate == \"smth was changed\":\n",
    "        score = sentence_bleu([msg], candidate, weights=(1./3, 1./3, 1./3, 0))\n",
    "    else:\n",
    "        score = sentence_bleu([msg], candidate, weights=(1, 0, 0, 0))\n",
    "        \n",
    "#     if score > 0.0001:\n",
    "#         print(\"Score: {:.2f}; Candidate: {:>16} Msg: {}\".format(score, candidate, msg))\n",
    "    total_score += score\n",
    "\n",
    "print()\n",
    "print(f\"Result = {total_score / len(msg_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### get data; AURORA ###################\n",
    "parent_dir = \"/Users/natalia.murycheva/Documents/gitCommitMessageCollectorStorage\"\n",
    "git_dir_name = \"aurora\"\n",
    "git_dir = os.path.join(parent_dir, git_dir_name)\n",
    "json_with_diffs = f\"{git_dir_name}_diff_blobs.json\"\n",
    "json_with_diffs = os.path.join(parent_dir, json_with_diffs)\n",
    "\n",
    "raw_data = load_and_filter_data(json_with_diffs)\n",
    "aurora = cluster_all_messages(raw_data)\n",
    "#print(aurora)\n",
    "# This commit was manufactured by cvs2svn to create branch 'testing'.\n",
    "# This commit was manufactured by cvs2svn to create branch 'test'.\n",
    "# This commit was manufactured by cvs2svn to create branch 'test_for_ven'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total shape \t\t(172323, 3)\n",
      "size cluster1 \tNoInfo\t138\t%0.08008217127139151\n",
      "size cluster2 \tTest\t1241\t%0.7201592358536005\n",
      "size cluster3 \tBomb\t60\t%0.03481833533538762\n",
      "size cluster4 \tNpe\t169\t%0.09807164452800844\n",
      "size cluster5 \tOther\t170715\t%99.06686861301162\n"
     ]
    }
   ],
   "source": [
    "print(f\"total shape \\t\\t{aurora.shape}\")\n",
    "\n",
    "total_size = aurora.shape[0]\n",
    "\n",
    "size1 = (aurora[cluster_header] == cluster1).sum()\n",
    "size2 = (aurora[cluster_header] == cluster2).sum()\n",
    "size3 = (aurora[cluster_header] == cluster3).sum()\n",
    "size4 = (aurora[cluster_header] == cluster4).sum()\n",
    "size5 = (aurora[cluster_header] == cluster5).sum()\n",
    "\n",
    "print(f\"size cluster1 \\t{cluster1}\\t{size1}\\t%{(size1/total_size)*100}\")\n",
    "print(f\"size cluster2 \\t{cluster2}\\t{size2}\\t%{(size2/total_size)*100}\")\n",
    "print(f\"size cluster3 \\t{cluster3}\\t{size3}\\t%{(size3/total_size)*100}\")\n",
    "print(f\"size cluster4 \\t{cluster4}\\t{size4}\\t%{(size4/total_size)*100}\")\n",
    "print(f\"size cluster5 \\t{cluster5}\\t{size5}\\t%{(size5/total_size)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### split data ###################\n",
    "\n",
    "a_msg_train , a_msg_test, a_y_train, a_y_test = train_test_split(aurora[message_header], aurora[cluster_header],\n",
    "                                                                 test_size=0.3, random_state=742)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       0.00      0.00      0.00        17\n",
      "      NoInfo       0.83      0.65      0.73        31\n",
      "         Npe       0.00      0.00      0.00        45\n",
      "       Other       0.99      1.00      1.00     51225\n",
      "        Test       0.75      0.24      0.36       379\n",
      "\n",
      "    accuracy                           0.99     51697\n",
      "   macro avg       0.52      0.38      0.42     51697\n",
      "weighted avg       0.99      0.99      0.99     51697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################### classififcation results ###################\n",
    "a_predicted = text_clf.predict(a_msg_test)\n",
    "np.mean(a_predicted == a_y_test)    \n",
    "print(metrics.classification_report(a_y_test, a_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = 0.03056921293487318\n"
     ]
    }
   ],
   "source": [
    "################### bleu score ###################\n",
    "total_score = 0.\n",
    "for msg, cluster in zip(a_msg_test, predicted):\n",
    "    if cluster == cluster1:\n",
    "        candidate = \"cleanup\"\n",
    "    elif cluster == cluster2:\n",
    "        candidate = \"test\"\n",
    "    elif cluster == cluster3:\n",
    "        candidate = \"bomb\"\n",
    "    elif cluster == cluster4:\n",
    "        candidate = \"npe\"\n",
    "    else:\n",
    "        candidate = \"smth was changed\"\n",
    "    \n",
    "    if candidate == \"smth was changed\":\n",
    "        score = sentence_bleu([msg], candidate, weights=(1./3, 1./3, 1./3, 0))\n",
    "    else:\n",
    "        score = sentence_bleu([msg], candidate, weights=(1, 0, 0, 0))\n",
    "    total_score += score\n",
    "\n",
    "print(f\"Result = {total_score / len(a_msg_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################### fit on aurora dataset ###################\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(a_msg_train, a_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       1.00      0.41      0.58        17\n",
      "      NoInfo       1.00      0.68      0.81        31\n",
      "         Npe       0.85      0.78      0.81        45\n",
      "       Other       1.00      0.98      0.99     51225\n",
      "        Test       0.23      0.76      0.35       379\n",
      "\n",
      "    accuracy                           0.98     51697\n",
      "   macro avg       0.82      0.72      0.71     51697\n",
      "weighted avg       0.99      0.98      0.98     51697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################### classififcation results ###################\n",
    "a_predicted = text_clf.predict(a_msg_test)\n",
    "np.mean(a_predicted == a_y_test)    \n",
    "print(metrics.classification_report(a_y_test, a_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = 0.03056921293487318\n"
     ]
    }
   ],
   "source": [
    "################### bleu score ###################\n",
    "total_score = 0.\n",
    "for msg, cluster in zip(a_msg_test, predicted):\n",
    "    if cluster == cluster1:\n",
    "        candidate = \"cleanup\"\n",
    "    elif cluster == cluster2:\n",
    "        candidate = \"test\"\n",
    "    elif cluster == cluster3:\n",
    "        candidate = \"bomb\"\n",
    "    elif cluster == cluster4:\n",
    "        candidate = \"npe\"\n",
    "    else:\n",
    "        candidate = \"smth was changed\"\n",
    "    \n",
    "#     if candidate == \"smth was changed\":\n",
    "#         score = sentence_bleu([msg], candidate, weights=(1./3, 1./3, 1./3, 0))\n",
    "#     else:\n",
    "    score = sentence_bleu([msg], candidate, weights=(1, 0, 0, 0))\n",
    "    total_score += score\n",
    "\n",
    "print(f\"Result = {total_score / len(a_msg_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################### fit on idea and then on aurora ###################\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(msg_train, y_train)\n",
    "text_clf.fit(a_msg_train, a_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       1.00      0.41      0.58        17\n",
      "      NoInfo       1.00      0.68      0.81        31\n",
      "         Npe       0.85      0.78      0.81        45\n",
      "       Other       1.00      0.98      0.99     51225\n",
      "        Test       0.23      0.76      0.35       379\n",
      "\n",
      "    accuracy                           0.98     51697\n",
      "   macro avg       0.82      0.72      0.71     51697\n",
      "weighted avg       0.99      0.98      0.98     51697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################### classififcation results ###################\n",
    "a_predicted = text_clf.predict(a_msg_test)\n",
    "np.mean(a_predicted == a_y_test)    \n",
    "print(metrics.classification_report(a_y_test, a_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = 0.03056921293487318\n"
     ]
    }
   ],
   "source": [
    "################### bleu score ###################\n",
    "total_score = 0.\n",
    "for msg, cluster in zip(a_msg_test, predicted):\n",
    "    if cluster == cluster1:\n",
    "        candidate = \"cleanup\"\n",
    "    elif cluster == cluster2:\n",
    "        candidate = \"test\"\n",
    "    elif cluster == cluster3:\n",
    "        candidate = \"bomb\"\n",
    "    elif cluster == cluster4:\n",
    "        candidate = \"npe\"\n",
    "    else:\n",
    "        candidate = \"smth was changed\"\n",
    "    \n",
    "    if candidate == \"smth was changed\":\n",
    "        score = sentence_bleu([msg], candidate, weights=(1./3, 1./3, 1./3, 0))\n",
    "    else:\n",
    "        score = sentence_bleu([msg], candidate, weights=(1, 0, 0, 0))\n",
    "    total_score += score\n",
    "\n",
    "print(f\"Result = {total_score / len(a_msg_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = 0.030483569957176968\n"
     ]
    }
   ],
   "source": [
    "################### bleu score; change cluster1 ###################\n",
    "total_score = 0.\n",
    "for msg, cluster in zip(a_msg_test, predicted):\n",
    "    if cluster == cluster1:\n",
    "        candidate = \"fix\"\n",
    "    elif cluster == cluster2:\n",
    "        candidate = \"test\"\n",
    "    elif cluster == cluster3:\n",
    "        candidate = \"bomb\"\n",
    "    elif cluster == cluster4:\n",
    "        candidate = \"npe\"\n",
    "    else:\n",
    "        candidate = \"smth was changed\"\n",
    "    \n",
    "    score = sentence_bleu([msg], candidate, weights=(1, 0, 0, 0))\n",
    "    total_score += score\n",
    "\n",
    "print(f\"Result = {total_score / len(a_msg_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
