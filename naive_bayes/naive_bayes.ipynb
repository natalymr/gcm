{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Dict\n",
    "from collections import OrderedDict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_header, tokens_header, message_header = \"cluster\", \"tokens\", \"message\"\n",
    "cluster1, cluster2, cluster3, cluster4, cluster5 = \"NoInfo\", \"Test\", \"Bomb\", \"Npe\", \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_data(input_file: str):\n",
    "    with open(input_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    elements_to_delete = []\n",
    "    for each in data:\n",
    "        if each[message_header] == \"(no message)\" or \\\n",
    "        each[message_header].startswith(\"This commit was manufactured by cvs2svn\"):\n",
    "            elements_to_delete.append(each)\n",
    "    for e in elements_to_delete:\n",
    "        data.remove(e)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def cluster_all_messages(raw_data: List[Dict]) -> DataFrame:\n",
    "    result = []\n",
    "    deleted_tokens, added_tokens = \"deletedTokens\", \"addedTokens\"\n",
    "    other_cluster_member_number = 0\n",
    "    for current in raw_data:\n",
    "        if \"cleanup\" in current[message_header] or \"minor\" in current[message_header] or \\\n",
    "            current[message_header] == \"fix\" or current[message_header] == \"fixes\" or \\\n",
    "            current[message_header] == \"several fixes\" or current[message_header] == \"minor fixes\" or \\\n",
    "            current[message_header] == \"yet another inference bug\" or \\\n",
    "            current[message_header] == \"some improvements (igor e. mikhailuk)\" or \\\n",
    "            current[message_header] == \"no changes actually\" or \\\n",
    "            current[message_header] == \"clean up\" or current[message_header] == \"cosmetics\":\n",
    "            \n",
    "            cluster = cluster1\n",
    "        elif \"test\" in current[message_header]:\n",
    "            cluster = cluster2\n",
    "        elif \"bomb\" in current[message_header]:\n",
    "            cluster = cluster3\n",
    "        elif \"npe\" in current[message_header] or \"null\" in current[message_header]:\n",
    "            cluster = cluster4\n",
    "        else:\n",
    "            if other_cluster_member_number >= 40_000:\n",
    "                continue\n",
    "            cluster = cluster5\n",
    "            other_cluster_member_number += 1\n",
    "            \n",
    "        \n",
    "        if current[deleted_tokens]:  # list with tokens not empty\n",
    "            result.append({cluster_header: cluster,\n",
    "                           tokens_header: current[deleted_tokens],\n",
    "                           message_header: current[message_header]\n",
    "                           })\n",
    "        if current[added_tokens]:  # list with tokens not empty\n",
    "            result.append({cluster_header: cluster,\n",
    "                       tokens_header: current[added_tokens],\n",
    "                       message_header: current[message_header]\n",
    "                       })\n",
    "    return DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All tokens \n",
    "### except `\"{\", \"}\", \"(\", \")\", \"[\", \"]\", \";\", \".\", \",\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### get data; IDEA ###################\n",
    "parent_dir = \"/Users/natalia.murycheva/Documents/gitCommitMessageCollectorStorage\"\n",
    "git_dir_name = \"intellij\"\n",
    "git_dir = os.path.join(parent_dir, git_dir_name)\n",
    "json_with_diffs = f\"{git_dir_name}_diff_blobs.json\"\n",
    "json_with_diffs = os.path.join(parent_dir, json_with_diffs)\n",
    "\n",
    "raw_data = load_and_filter_data(json_with_diffs)\n",
    "df = cluster_all_messages(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total shape \t\t(63516, 3)\n",
      "size cluster1 \tNoInfo\t4048\t%6.37319730461616\n",
      "size cluster2 \tTest\t3980\t%6.26613766609988\n",
      "size cluster3 \tBomb\t423\t%0.6659739278292084\n",
      "size cluster4 \tNpe\t647\t%1.0186409723534229\n",
      "size cluster5 \tOther\t54418\t%85.67605012910133\n"
     ]
    }
   ],
   "source": [
    "print(f\"total shape \\t\\t{df.shape}\")\n",
    "\n",
    "total_size = df.shape[0]\n",
    "\n",
    "size1 = (df[cluster_header] == cluster1).sum()\n",
    "size2 = (df[cluster_header] == cluster2).sum()\n",
    "size3 = (df[cluster_header] == cluster3).sum()\n",
    "size4 = (df[cluster_header] == cluster4).sum()\n",
    "size5 = (df[cluster_header] == cluster5).sum()\n",
    "\n",
    "print(f\"size cluster1 \\t{cluster1}\\t{size1}\\t%{(size1/total_size)*100}\")\n",
    "print(f\"size cluster2 \\t{cluster2}\\t{size2}\\t%{(size2/total_size)*100}\")\n",
    "print(f\"size cluster3 \\t{cluster3}\\t{size3}\\t%{(size3/total_size)*100}\")\n",
    "print(f\"size cluster4 \\t{cluster4}\\t{size4}\\t%{(size4/total_size)*100}\")\n",
    "print(f\"size cluster5 \\t{cluster5}\\t{size5}\\t%{(size5/total_size)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### split data ###################\n",
    "\n",
    "msg_train , msg_test, y_train, y_test = train_test_split(df[message_header], df[cluster_header],\n",
    "                                                         test_size=0.3, random_state=142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=0.95,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.95)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(msg_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       0.98      0.45      0.62       135\n",
      "      NoInfo       0.98      0.91      0.94      1183\n",
      "         Npe       1.00      0.59      0.74       193\n",
      "       Other       0.98      1.00      0.99     16289\n",
      "        Test       0.94      0.84      0.89      1255\n",
      "\n",
      "    accuracy                           0.97     19055\n",
      "   macro avg       0.97      0.76      0.83     19055\n",
      "weighted avg       0.97      0.97      0.97     19055\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################### classififcation results ###################\n",
    "predicted = text_clf.predict(msg_test)\n",
    "np.mean(predicted == y_test)    \n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cluster = NoInfo\n",
      "{56: ['minor'], 54: ['cleanup'], 51: ['some'], 40: ['cmpletion', 'and', 'xmlmeta', 'enchantment'], 21: ['to'], 20: ['improvements'], 11: ['code', 'of'], 10: ['for', 'fixes', 'imporved', 'performance', 'formatter', 'smartencodinginputstream', 'file', 'reading', 'routines', 'implementation'], 4: ['fix', 'changes', 'vf'], 3: ['added', 'methods', 'internal', 'use', 'information', 'optimization'], 2: ['cosmetics'], 1: ['corrected', 'bug', 'in', 'example', 'fixed', 'issue', 'with', 'length', 'an', 'array', 'memory', 'leak', 'change', 'fabrique', 'update', 'version', '0', '3', 'display', 'purposes']}\n",
      "\n",
      "Current cluster = Test\n",
      "{401: ['tests'], 362: ['test'], 264: ['replace'], 263: ['to'], 262: ['testing'], 247: ['support'], 244: ['the'], 242: ['first'], 238: ['enable'], 237: ['statement', 'expression', 'implementation', 'used'], 188: ['for'], 116: ['fixed'], 112: ['added'], 101: ['of'], 100: ['in'], 99: ['and'], 78: ['xml'], 75: ['completion'], 72: ['initial'], 70: ['new'], 64: ['case', 'checkin', 'license'], 57: ['networking'], 48: ['fix'], 38: ['handling', 'separate'], 37: ['some']}\n",
      "\n",
      "Current cluster = Bomb\n",
      "{18: ['time', 'bomb'], 12: ['timebomb'], 5: ['fixed'], 3: ['proper', 'timebombing', 'with', 'ideatestutil', 'bombexplodes'], 1: ['no', 'root', 'tag', 'timebombed', 'test']}\n",
      "\n",
      "Current cluster = Npe\n",
      "{110: ['null'], 59: ['fixed'], 47: ['npe'], 44: ['in'], 36: ['for'], 34: ['returns'], 33: ['on', 'how'], 31: ['getmodulefile'], 15: ['with'], 13: ['is', 'type'], 12: ['file', 'added', 'substitutor'], 11: ['parameters'], 10: ['property'], 9: ['issue'], 8: ['check', 'fix', 'reference'], 7: ['containing', 'element', 'there', 'case', 'when', 'fixes', '1', 'bug', 'diagnostics', 'undeclared', 'were', 'accepted', 'phantom', 'rules', '2', 'special', 'values', 'return'], 6: ['of', 'psiclasstype', 'isassignablefrom', 'use', 'its', 'own', 'isinheritor', 'also', 'some', 'resolve', 'now', 'can', 'no', 'mapping', 'parameter'], 5: ['to', 'descriptor', 'select', 'word', 'control', 'q', 'css', 'value', 'shows', 'help', 'caching'], 4: ['not', 'name', 'completing', 'after', 'method', 'call'], 3: ['virtual', 'this', 'tag', 'selection', 'be', 'project'], 2: ['does', 'removed', 'extra', 'psifile', 'from', 'context', 'returned', 'inserted', 'during', 'e', 'g', 'istypedvar', 'xmlfile', 'that', 'attribute', 'editor', 'patternnode', 'still', 'do', 'know', 'it', 'will', 'predicate', 'psielement', 'configuration', 'completion', 'problem'], 1: ['checkifcached', 'complain', 'assert', 'got', 'data', 'by', 'add', 'trace', 'derived', 'binding', 'validator', 'messages', 'tree', 'empty', 'class', 'typing', 'psi', 'handled', 'correctly', 'p2', 'more', 'background', 'color', 'has', 'nsdescriptor', 'support', 'capabilities', 'document', 'dtd', 'windows', 'retrieval', 'cancel', 'super', 'dialog', 'getcontainingfile', 'checks', 'under', 'a', 'buffer', 'ixed', 'cosmetic', 'reduntant', 'assignment', 'children', 'directory', 'very', 'rarely', 'race', 'conditions', 'could', 'updatehighlighters', 'compiling', 'possible', 'getusages', 'usageinfo', 'processpsitreechanged', 'the', 'so', 'all', 'text', 'fields', 'usage', 'view', '31434', 'contains', 'ejbs', 'pallada', 'ipr', 'then', 'getejbmoduleproperties', 'regress', 'passed', 'createjavahighlighter', 'myservletmappingview', '29782', 'inspectiongadgets', 'plugin', 'sometimes', 'as', 'description', 'template', 'used', 'cause', 'exporting', 'inspection', 'results', 'html', 'root', 'package', 'assertion', 'map', 'jsp', 'getbaseclass', 'infos', 'instead', 'array', 'intensions', 'xml']}\n",
      "\n",
      "Current cluster = Other\n",
      "{1845: ['in'], 1792: ['for'], 1480: ['fixed'], 1199: ['new'], 1162: ['of'], 968: ['to'], 904: ['and'], 880: ['with'], 782: ['completion'], 736: ['changes'], 727: ['xml'], 714: ['support'], 710: ['added'], 670: ['fix'], 601: ['resolve'], 567: ['scr'], 506: ['fixes'], 468: ['some'], 459: ['structure'], 453: ['css'], 406: ['initial'], 394: ['file'], 390: ['jsp'], 340: ['code'], 323: ['fixing']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def invert_dict(input_dict: Dict[str, int]) -> Dict[int, List[str]]:\n",
    "    result = dict()\n",
    "\n",
    "    for key, value in input_dict.items():\n",
    "        if value not in result:\n",
    "            result[value] = [key]\n",
    "        else:\n",
    "            result[value].append(key)\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_top_popular_words_per_class(raw_data: List[Dict]) -> DataFrame:\n",
    "    result = {}\n",
    "    for current in raw_data:\n",
    "        if \"cleanup\" in current[message_header] or \"minor\" in current[message_header] or \\\n",
    "            current[message_header] == \"fix\" or current[message_header] == \"fixes\" or \\\n",
    "            current[message_header] == \"several fixes\" or current[message_header] == \"minor fixes\" or \\\n",
    "            current[message_header] == \"yet another inference bug\" or \\\n",
    "            current[message_header] == \"some improvements (igor e. mikhailuk)\" or \\\n",
    "            current[message_header] == \"no changes actually\" or \\\n",
    "            current[message_header] == \"clean up\" or current[message_header] == \"cosmetics\":\n",
    "\n",
    "            cluster = cluster1\n",
    "        elif \"test\" in current[message_header]:\n",
    "            cluster = cluster2\n",
    "        elif \"bomb\" in current[message_header]:\n",
    "            cluster = cluster3\n",
    "        elif \"npe\" in current[message_header] or \"null\" in current[message_header]:\n",
    "            cluster = cluster4\n",
    "        else:\n",
    "            cluster = cluster5\n",
    "            \n",
    "        if cluster not in result:\n",
    "            result[cluster] = []\n",
    "        else:\n",
    "            result[cluster].append(current[message_header])\n",
    "        \n",
    "    for cur_cluster in [cluster1, cluster2, cluster3, cluster4, cluster5]:\n",
    "        print(f\"Current cluster = {cur_cluster}\")\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(result[cur_cluster])\n",
    "        counts_vs_word = invert_dict(tokenizer.word_counts)\n",
    "        counts_vs_word_sorted = OrderedDict(sorted(counts_vs_word.items(), reverse=True))\n",
    "        top_popular_words = dict(itertools.islice(counts_vs_word_sorted.items(), 0, 25))\n",
    "        print(top_popular_words)\n",
    "        print()\n",
    "        \n",
    "get_top_popular_words_per_class(raw_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster1, cluster2, cluster3, cluster4, cluster5 = \"NoInfo\", \"Test\", \"Bomb\", \"Npe\", \"Other\"\n",
    "candidate1 = \"cleanup\"\n",
    "candidate2 = \"test\"\n",
    "candidate3 = \"timebomb\"\n",
    "candidate4 = \"npe fix and null checks removed\"\n",
    "candidate5 = \"refactoring and ideadev fix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result = 0.037946796158592935\n",
      "Number of bad score 17569; test size 19055\n",
      "Elements number per cluster {'NoInfo': 1102, 'Test': 1124, 'Bomb': 62, 'Npe': 113, 'Other': 16654}\n",
      "Elements number with bad score per cluster {'NoInfo': 315, 'Test': 437, 'Bomb': 50, 'Npe': 113, 'Other': 16654}\n"
     ]
    }
   ],
   "source": [
    "################### bleu score; IDEA ###################\n",
    "total_score = 0.\n",
    "bad_score_number = 0\n",
    "cluster_vs_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "cluster_vs_bad_score_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "\n",
    "for msg, cluster in zip(msg_test, predicted):\n",
    "    cluster_vs_number[cluster] += 1\n",
    "    \n",
    "    if cluster == cluster1:\n",
    "        candidate = candidate1\n",
    "    elif cluster == cluster2:\n",
    "        candidate = candidate2\n",
    "    elif cluster == cluster3:\n",
    "        candidate = candidate3\n",
    "    elif cluster == cluster4:\n",
    "        candidate = candidate4\n",
    "    else:\n",
    "        candidate = candidate5\n",
    "    \n",
    "    score = sentence_bleu([msg], candidate, weights=((1./len(candidate),) * len(candidate)))\n",
    "        \n",
    "    if score < 0.001:\n",
    "        bad_score_number += 1\n",
    "        cluster_vs_bad_score_number[cluster] += 1\n",
    "#         if cluster == cluster4:\n",
    "#             print(\"Score: {:.6f}; Candidate: {:>16}; Msg: {}\".format(score, candidate, msg))\n",
    "    total_score += score\n",
    "\n",
    "print()\n",
    "print(f\"Result = {total_score / len(msg_test)}\")\n",
    "print(f\"Number of bad score {bad_score_number}; test size {len(msg_test)}\")\n",
    "print(f\"Elements number per cluster {cluster_vs_number}\")\n",
    "print(f\"Elements number with bad score per cluster {cluster_vs_bad_score_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### get data; AURORA ###################\n",
    "parent_dir = \"/Users/natalia.murycheva/Documents/gitCommitMessageCollectorStorage\"\n",
    "git_dir_name = \"aurora\"\n",
    "git_dir = os.path.join(parent_dir, git_dir_name)\n",
    "json_with_diffs = f\"{git_dir_name}_diff_blobs.json\"\n",
    "json_with_diffs = os.path.join(parent_dir, json_with_diffs)\n",
    "\n",
    "raw_data = load_and_filter_data(json_with_diffs)\n",
    "aurora = cluster_all_messages(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aurora sample number \t\t(19453, 3)\n",
      "size cluster1 \tNoInfo\t140\t%0.7196833393306945\n",
      "size cluster2 \tTest\t1332\t%6.847272914203464\n",
      "size cluster3 \tBomb\t60\t%0.3084357168560119\n",
      "size cluster4 \tNpe\t169\t%0.868760602477767\n",
      "size cluster5 \tOther\t17752\t%91.25584742713207\n"
     ]
    }
   ],
   "source": [
    "print(f\"Aurora sample number \\t\\t{aurora.shape}\")\n",
    "\n",
    "total_size = aurora.shape[0]\n",
    "\n",
    "size1 = (aurora[cluster_header] == cluster1).sum()\n",
    "size2 = (aurora[cluster_header] == cluster2).sum()\n",
    "size3 = (aurora[cluster_header] == cluster3).sum()\n",
    "size4 = (aurora[cluster_header] == cluster4).sum()\n",
    "size5 = (aurora[cluster_header] == cluster5).sum()\n",
    "\n",
    "print(f\"size cluster1 \\t{cluster1}\\t{size1}\\t%{(size1/total_size)*100}\")\n",
    "print(f\"size cluster2 \\t{cluster2}\\t{size2}\\t%{(size2/total_size)*100}\")\n",
    "print(f\"size cluster3 \\t{cluster3}\\t{size3}\\t%{(size3/total_size)*100}\")\n",
    "print(f\"size cluster4 \\t{cluster4}\\t{size4}\\t%{(size4/total_size)*100}\")\n",
    "print(f\"size cluster5 \\t{cluster5}\\t{size5}\\t%{(size5/total_size)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### split data ###################\n",
    "\n",
    "a_msg_train , a_msg_test, a_y_train, a_y_test = train_test_split(aurora[message_header], aurora[cluster_header],\n",
    "                                                                 test_size=0.3, random_state=742)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       1.00      0.23      0.38        13\n",
      "      NoInfo       0.78      0.89      0.83        47\n",
      "         Npe       1.00      0.29      0.45        51\n",
      "       Other       0.94      0.96      0.95      5303\n",
      "        Test       0.47      0.41      0.44       422\n",
      "\n",
      "    accuracy                           0.91      5836\n",
      "   macro avg       0.84      0.56      0.61      5836\n",
      "weighted avg       0.91      0.91      0.91      5836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################### classififcation results ###################\n",
    "a_predicted = text_clf.predict(a_msg_test)\n",
    "np.mean(a_predicted == a_y_test)    \n",
    "print(metrics.classification_report(a_y_test, a_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result = 0.014686711258301514\n",
      "Number of bad score 5653; test size 5836\n",
      "Elements number per cluster {'NoInfo': 54, 'Test': 365, 'Bomb': 3, 'Npe': 15, 'Other': 5399}\n",
      "Elements number with bad score per cluster {'NoInfo': 30, 'Test': 209, 'Bomb': 0, 'Npe': 15, 'Other': 5399}\n"
     ]
    }
   ],
   "source": [
    "################### bleu score; AURORA ###################\n",
    "total_score = 0.\n",
    "bad_score_number = 0\n",
    "cluster_vs_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "cluster_vs_bad_score_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "\n",
    "for msg, cluster in zip(a_msg_test, a_predicted):\n",
    "    cluster_vs_number[cluster] += 1\n",
    "    \n",
    "    if cluster == cluster1:\n",
    "        candidate = candidate1\n",
    "    elif cluster == cluster2:\n",
    "        candidate = candidate2\n",
    "    elif cluster == cluster3:\n",
    "        candidate = candidate3\n",
    "    elif cluster == cluster4:\n",
    "        candidate = candidate4\n",
    "    else:\n",
    "        candidate = candidate5\n",
    "    \n",
    "    score = sentence_bleu([msg], candidate, weights=((1./len(candidate),) * len(candidate)))\n",
    "        \n",
    "    if score < 0.001:\n",
    "        bad_score_number += 1\n",
    "        cluster_vs_bad_score_number[cluster] += 1\n",
    "#         if cluster == cluster4:\n",
    "#             print(\"Score: {:.6f}; Candidate: {:>16}; Msg: {}\".format(score, candidate, msg))\n",
    "    total_score += score\n",
    "\n",
    "print()\n",
    "print(f\"Result = {total_score / len(a_msg_test)}\")\n",
    "print(f\"Number of bad score {bad_score_number}; test size {len(a_msg_test)}\")\n",
    "print(f\"Elements number per cluster {cluster_vs_number}\")\n",
    "print(f\"Elements number with bad score per cluster {cluster_vs_bad_score_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=0.95,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################### fit on aurora dataset ###################\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.95)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(a_msg_train, a_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       1.00      0.54      0.70        13\n",
      "      NoInfo       1.00      0.51      0.68        47\n",
      "         Npe       1.00      0.37      0.54        51\n",
      "       Other       0.95      1.00      0.98      5303\n",
      "        Test       0.99      0.52      0.68       422\n",
      "\n",
      "    accuracy                           0.95      5836\n",
      "   macro avg       0.99      0.59      0.72      5836\n",
      "weighted avg       0.96      0.95      0.95      5836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################### classififcation results ###################\n",
    "a_predicted = text_clf.predict(a_msg_test)\n",
    "np.mean(a_predicted == a_y_test)    \n",
    "print(metrics.classification_report(a_y_test, a_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result = 0.01288976238844559\n",
      "Number of bad score 5704; test size 5836\n",
      "Elements number per cluster {'NoInfo': 24, 'Test': 224, 'Bomb': 7, 'Npe': 19, 'Other': 5562}\n",
      "Elements number with bad score per cluster {'NoInfo': 1, 'Test': 115, 'Bomb': 7, 'Npe': 19, 'Other': 5562}\n"
     ]
    }
   ],
   "source": [
    "################### bleu score; AURORA ###################\n",
    "total_score = 0.\n",
    "bad_score_number = 0\n",
    "cluster_vs_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "cluster_vs_bad_score_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "\n",
    "for msg, cluster in zip(a_msg_test, a_predicted):\n",
    "    cluster_vs_number[cluster] += 1\n",
    "    \n",
    "    if cluster == cluster1:\n",
    "        candidate = candidate1\n",
    "    elif cluster == cluster2:\n",
    "        candidate = candidate2\n",
    "    elif cluster == cluster3:\n",
    "        candidate = candidate3\n",
    "    elif cluster == cluster4:\n",
    "        candidate = candidate4\n",
    "    else:\n",
    "        candidate = candidate5\n",
    "    \n",
    "    score = sentence_bleu([msg], candidate, weights=((1./len(candidate),) * len(candidate)))\n",
    "        \n",
    "    if score < 0.001:\n",
    "        bad_score_number += 1\n",
    "        cluster_vs_bad_score_number[cluster] += 1\n",
    "#         if cluster == cluster4:\n",
    "#             print(\"Score: {:.6f}; Candidate: {:>16}; Msg: {}\".format(score, candidate, msg))\n",
    "    total_score += score\n",
    "\n",
    "print()\n",
    "print(f\"Result = {total_score / len(a_msg_test)}\")\n",
    "print(f\"Number of bad score {bad_score_number}; test size {len(a_msg_test)}\")\n",
    "print(f\"Elements number per cluster {cluster_vs_number}\")\n",
    "print(f\"Elements number with bad score per cluster {cluster_vs_bad_score_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=0.95,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################### fit on idea and then on aurora ###################\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.95)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(msg_train, y_train)\n",
    "text_clf.fit(a_msg_train, a_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       1.00      0.54      0.70        13\n",
      "      NoInfo       1.00      0.51      0.68        47\n",
      "         Npe       1.00      0.37      0.54        51\n",
      "       Other       0.95      1.00      0.98      5303\n",
      "        Test       0.99      0.52      0.68       422\n",
      "\n",
      "    accuracy                           0.95      5836\n",
      "   macro avg       0.99      0.59      0.72      5836\n",
      "weighted avg       0.96      0.95      0.95      5836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################### classififcation results ###################\n",
    "a_predicted = text_clf.predict(a_msg_test)\n",
    "np.mean(a_predicted == a_y_test)    \n",
    "print(metrics.classification_report(a_y_test, a_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result = 0.01288976238844559\n",
      "Number of bad score 5704; test size 5836\n",
      "Elements number per cluster {'NoInfo': 24, 'Test': 224, 'Bomb': 7, 'Npe': 19, 'Other': 5562}\n",
      "Elements number with bad score per cluster {'NoInfo': 1, 'Test': 115, 'Bomb': 7, 'Npe': 19, 'Other': 5562}\n"
     ]
    }
   ],
   "source": [
    "################### bleu score; AURORA ###################\n",
    "total_score = 0.\n",
    "bad_score_number = 0\n",
    "cluster_vs_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "cluster_vs_bad_score_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "\n",
    "for msg, cluster in zip(a_msg_test, a_predicted):\n",
    "    cluster_vs_number[cluster] += 1\n",
    "    \n",
    "    if cluster == cluster1:\n",
    "        candidate = candidate1\n",
    "    elif cluster == cluster2:\n",
    "        candidate = candidate2\n",
    "    elif cluster == cluster3:\n",
    "        candidate = candidate3\n",
    "    elif cluster == cluster4:\n",
    "        candidate = candidate4\n",
    "    else:\n",
    "        candidate = candidate5\n",
    "    \n",
    "    score = sentence_bleu([msg], candidate, weights=((1./len(candidate),) * len(candidate)))\n",
    "        \n",
    "    if score < 0.001:\n",
    "        bad_score_number += 1\n",
    "        cluster_vs_bad_score_number[cluster] += 1\n",
    "#         if cluster == cluster4:\n",
    "#             print(\"Score: {:.6f}; Candidate: {:>16}; Msg: {}\".format(score, candidate, msg))\n",
    "    total_score += score\n",
    "\n",
    "print()\n",
    "print(f\"Result = {total_score / len(a_msg_test)}\")\n",
    "print(f\"Number of bad score {bad_score_number}; test size {len(a_msg_test)}\")\n",
    "print(f\"Elements number per cluster {cluster_vs_number}\")\n",
    "print(f\"Elements number with bad score per cluster {cluster_vs_bad_score_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result = 0.037946796158592935\n",
      "Number of bad score 17569; test size 19055\n",
      "Elements number per cluster {'NoInfo': 1102, 'Test': 1124, 'Bomb': 62, 'Npe': 113, 'Other': 16654}\n",
      "Elements number with bad score per cluster {'NoInfo': 315, 'Test': 437, 'Bomb': 50, 'Npe': 113, 'Other': 16654}\n"
     ]
    }
   ],
   "source": [
    "################### bleu score; IDEA ###################\n",
    "total_score = 0.\n",
    "bad_score_number = 0\n",
    "cluster_vs_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "cluster_vs_bad_score_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "\n",
    "for msg, cluster in zip(msg_test, predicted):\n",
    "    cluster_vs_number[cluster] += 1\n",
    "    \n",
    "    if cluster == cluster1:\n",
    "        candidate = candidate1\n",
    "    elif cluster == cluster2:\n",
    "        candidate = candidate2\n",
    "    elif cluster == cluster3:\n",
    "        candidate = candidate3\n",
    "    elif cluster == cluster4:\n",
    "        candidate = candidate4\n",
    "    else:\n",
    "        candidate = candidate5\n",
    "    \n",
    "    score = sentence_bleu([msg], candidate, weights=((1./len(candidate),) * len(candidate)))\n",
    "        \n",
    "    if score < 0.001:\n",
    "        bad_score_number += 1\n",
    "        cluster_vs_bad_score_number[cluster] += 1\n",
    "#         if cluster == cluster4:\n",
    "#             print(\"Score: {:.6f}; Candidate: {:>16}; Msg: {}\".format(score, candidate, msg))\n",
    "    total_score += score\n",
    "\n",
    "print()\n",
    "print(f\"Result = {total_score / len(msg_test)}\")\n",
    "print(f\"Number of bad score {bad_score_number}; test size {len(msg_test)}\")\n",
    "print(f\"Elements number per cluster {cluster_vs_number}\")\n",
    "print(f\"Elements number with bad score per cluster {cluster_vs_bad_score_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### get data; IDEA ###################\n",
    "parent_dir = \"/Users/natalia.murycheva/Documents/gitCommitMessageCollectorStorage\"\n",
    "git_dir_name = \"intellij\"\n",
    "git_dir = os.path.join(parent_dir, git_dir_name)\n",
    "json_with_diffs = f\"{git_dir_name}_diff_blobs_identifiers.json\"\n",
    "json_with_diffs = os.path.join(parent_dir, json_with_diffs)\n",
    "\n",
    "raw_data = load_and_filter_data(json_with_diffs)\n",
<<<<<<< HEAD
    "aurora_id = cluster_all_messages(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aurora sample number \t\t(19028, 3)\n",
      "size cluster1 \tNoInfo\t137\t%0.7199915913390792\n",
      "size cluster2 \tTest\t1317\t%6.921379020391003\n",
      "size cluster3 \tBomb\t55\t%0.28904771915072525\n",
      "size cluster4 \tNpe\t168\t%0.882909396678579\n",
      "size cluster5 \tOther\t17351\t%91.18667227244062\n"
     ]
    }
   ],
   "source": [
    "print(f\"Aurora sample number \\t\\t{aurora_id.shape}\")\n",
    "\n",
    "total_size = aurora_id.shape[0]\n",
    "\n",
    "size1 = (aurora_id[cluster_header] == cluster1).sum()\n",
    "size2 = (aurora_id[cluster_header] == cluster2).sum()\n",
    "size3 = (aurora_id[cluster_header] == cluster3).sum()\n",
    "size4 = (aurora_id[cluster_header] == cluster4).sum()\n",
    "size5 = (aurora_id[cluster_header] == cluster5).sum()\n",
    "\n",
    "print(f\"size cluster1 \\t{cluster1}\\t{size1}\\t%{(size1/total_size)*100}\")\n",
    "print(f\"size cluster2 \\t{cluster2}\\t{size2}\\t%{(size2/total_size)*100}\")\n",
    "print(f\"size cluster3 \\t{cluster3}\\t{size3}\\t%{(size3/total_size)*100}\")\n",
    "print(f\"size cluster4 \\t{cluster4}\\t{size4}\\t%{(size4/total_size)*100}\")\n",
    "print(f\"size cluster5 \\t{cluster5}\\t{size5}\\t%{(size5/total_size)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### split data ###################\n",
    "\n",
    "id_a_msg_train , id_a_msg_test, id_a_y_train, id_a_y_test = train_test_split(aurora_id[message_header], \n",
    "                                                                             aurora_id[cluster_header],\n",
    "                                                                             test_size=0.3, random_state=742)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       1.00      0.21      0.35        19\n",
      "      NoInfo       0.72      0.89      0.80        38\n",
      "         Npe       1.00      0.32      0.48        50\n",
      "       Other       0.95      0.99      0.97      5209\n",
      "        Test       0.70      0.40      0.51       393\n",
      "\n",
      "    accuracy                           0.94      5709\n",
      "   macro avg       0.87      0.56      0.62      5709\n",
      "weighted avg       0.93      0.94      0.93      5709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################### classififcation results ###################\n",
    "id_a_predicted = text_clf.predict(id_a_msg_test)\n",
    "np.mean(id_a_predicted == id_a_y_test)    \n",
    "print(metrics.classification_report(id_a_y_test, id_a_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result = 0.014575246380114253\n",
      "Number of bad score 5541; test size 5709\n",
      "Elements number per cluster {'NoInfo': 47, 'Test': 223, 'Bomb': 4, 'Npe': 16, 'Other': 5419}\n",
      "Elements number with bad score per cluster {'NoInfo': 26, 'Test': 80, 'Bomb': 0, 'Npe': 16, 'Other': 5419}\n"
     ]
    }
   ],
   "source": [
    "################### bleu score; AURORA ###################\n",
    "total_score = 0.\n",
    "bad_score_number = 0\n",
    "cluster_vs_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "cluster_vs_bad_score_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "\n",
    "for msg, cluster in zip(id_a_msg_test, id_a_predicted):\n",
    "    cluster_vs_number[cluster] += 1\n",
    "    \n",
    "    if cluster == cluster1:\n",
    "        candidate = candidate1\n",
    "    elif cluster == cluster2:\n",
    "        candidate = candidate2\n",
    "    elif cluster == cluster3:\n",
    "        candidate = candidate3\n",
    "    elif cluster == cluster4:\n",
    "        candidate = candidate4\n",
    "    else:\n",
    "        candidate = candidate5\n",
    "    \n",
    "    score = sentence_bleu([msg], candidate, weights=((1./len(candidate),) * len(candidate)))\n",
    "        \n",
    "    if score < 0.001:\n",
    "        bad_score_number += 1\n",
    "        cluster_vs_bad_score_number[cluster] += 1\n",
    "#         if cluster == cluster4:\n",
    "#             print(\"Score: {:.6f}; Candidate: {:>16}; Msg: {}\".format(score, candidate, msg))\n",
    "    total_score += score\n",
    "\n",
    "print()\n",
    "print(f\"Result = {total_score / len(id_a_msg_test)}\")\n",
    "print(f\"Number of bad score {bad_score_number}; test size {len(id_a_msg_test)}\")\n",
    "print(f\"Elements number per cluster {cluster_vs_number}\")\n",
    "print(f\"Elements number with bad score per cluster {cluster_vs_bad_score_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=0.95,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################### fit on AURORA ###################\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.95)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(id_a_msg_train, id_a_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       0.00      0.00      0.00        19\n",
      "      NoInfo       1.00      0.68      0.81        38\n",
      "         Npe       1.00      0.40      0.57        50\n",
      "       Other       0.96      1.00      0.98      5209\n",
      "        Test       0.95      0.58      0.72       393\n",
      "\n",
      "    accuracy                           0.96      5709\n",
      "   macro avg       0.78      0.53      0.62      5709\n",
      "weighted avg       0.96      0.96      0.95      5709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################### classififcation results ###################\n",
    "id_a_predicted = text_clf.predict(id_a_msg_test)\n",
    "np.mean(id_a_predicted == id_a_y_test)    \n",
    "print(metrics.classification_report(id_a_y_test, id_a_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result = 0.01311591649689736\n",
      "Number of bad score 5586; test size 5709\n",
      "Elements number per cluster {'NoInfo': 26, 'Test': 240, 'Bomb': 0, 'Npe': 20, 'Other': 5423}\n",
      "Elements number with bad score per cluster {'NoInfo': 5, 'Test': 138, 'Bomb': 0, 'Npe': 20, 'Other': 5423}\n"
     ]
    }
   ],
   "source": [
    "################### bleu score; AURORA ###################\n",
    "total_score = 0.\n",
    "bad_score_number = 0\n",
    "cluster_vs_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "cluster_vs_bad_score_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "\n",
    "for msg, cluster in zip(id_a_msg_test, id_a_predicted):\n",
    "    cluster_vs_number[cluster] += 1\n",
    "    \n",
    "    if cluster == cluster1:\n",
    "        candidate = candidate1\n",
    "    elif cluster == cluster2:\n",
    "        candidate = candidate2\n",
    "    elif cluster == cluster3:\n",
    "        candidate = candidate3\n",
    "    elif cluster == cluster4:\n",
    "        candidate = candidate4\n",
    "    else:\n",
    "        candidate = candidate5\n",
    "    \n",
    "    score = sentence_bleu([msg], candidate, weights=((1./len(candidate),) * len(candidate)))\n",
    "        \n",
    "    if score < 0.001:\n",
    "        bad_score_number += 1\n",
    "        cluster_vs_bad_score_number[cluster] += 1\n",
    "#         if cluster == cluster4:\n",
    "#             print(\"Score: {:.6f}; Candidate: {:>16}; Msg: {}\".format(score, candidate, msg))\n",
    "    total_score += score\n",
    "\n",
    "print()\n",
    "print(f\"Result = {total_score / len(id_a_msg_test)}\")\n",
    "print(f\"Number of bad score {bad_score_number}; test size {len(id_a_msg_test)}\")\n",
    "print(f\"Elements number per cluster {cluster_vs_number}\")\n",
    "print(f\"Elements number with bad score per cluster {cluster_vs_bad_score_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=0.95,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################### fit on IDEA, AURORA ###################\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.95)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(id_msg_train, id_y_train)\n",
    "text_clf.fit(id_a_msg_train, id_a_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bomb       0.00      0.00      0.00        19\n",
      "      NoInfo       1.00      0.68      0.81        38\n",
      "         Npe       1.00      0.40      0.57        50\n",
      "       Other       0.96      1.00      0.98      5209\n",
      "        Test       0.95      0.58      0.72       393\n",
      "\n",
      "    accuracy                           0.96      5709\n",
      "   macro avg       0.78      0.53      0.62      5709\n",
      "weighted avg       0.96      0.96      0.95      5709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################### classififcation results ###################\n",
    "id_a_predicted = text_clf.predict(id_a_msg_test)\n",
    "np.mean(id_a_predicted == id_a_y_test)    \n",
    "print(metrics.classification_report(id_a_y_test, id_a_predicted))"
=======
    "df = cluster_all_messages(raw_data)"
>>>>>>> parent of 902f541... add naive bayes with dataset only with identifiers
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result = 0.01311591649689736\n",
      "Number of bad score 5586; test size 5709\n",
      "Elements number per cluster {'NoInfo': 26, 'Test': 240, 'Bomb': 0, 'Npe': 20, 'Other': 5423}\n",
      "Elements number with bad score per cluster {'NoInfo': 5, 'Test': 138, 'Bomb': 0, 'Npe': 20, 'Other': 5423}\n"
     ]
    }
   ],
   "source": [
    "################### bleu score; AURORA ###################\n",
    "total_score = 0.\n",
    "bad_score_number = 0\n",
    "cluster_vs_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "cluster_vs_bad_score_number = {cluster1: 0, cluster2: 0, cluster3: 0, cluster4: 0, cluster5: 0}\n",
    "\n",
    "for msg, cluster in zip(id_a_msg_test, id_a_predicted):\n",
    "    cluster_vs_number[cluster] += 1\n",
    "    \n",
    "    if cluster == cluster1:\n",
    "        candidate = candidate1\n",
    "    elif cluster == cluster2:\n",
    "        candidate = candidate2\n",
    "    elif cluster == cluster3:\n",
    "        candidate = candidate3\n",
    "    elif cluster == cluster4:\n",
    "        candidate = candidate4\n",
    "    else:\n",
    "        candidate = candidate5\n",
    "    \n",
    "    score = sentence_bleu([msg], candidate, weights=((1./len(candidate),) * len(candidate)))\n",
    "        \n",
    "    if score < 0.001:\n",
    "        bad_score_number += 1\n",
    "        cluster_vs_bad_score_number[cluster] += 1\n",
    "#         if cluster == cluster4:\n",
    "#             print(\"Score: {:.6f}; Candidate: {:>16}; Msg: {}\".format(score, candidate, msg))\n",
    "    total_score += score\n",
    "\n",
    "print()\n",
    "print(f\"Result = {total_score / len(id_a_msg_test)}\")\n",
    "print(f\"Number of bad score {bad_score_number}; test size {len(id_a_msg_test)}\")\n",
    "print(f\"Elements number per cluster {cluster_vs_number}\")\n",
    "print(f\"Elements number with bad score per cluster {cluster_vs_bad_score_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
